<!DOCTYPE html>
<html>
    <head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" >
    <title>
        
        非洲人的战斗-DinosaurusLand · SecretBase
        
    </title>
    <link rel="icon" href= /assets/favicon.ico>
    <!-- TODO: 在font-face加载完毕后改变字体  -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/webfont/1.6.28/webfontloader.js"></script>
    <!-- 提前加载place holder  -->
    <style type="text/css">
        @font-face {
            font-family: 'Oswald-Regular';
            src: url(/font/Oswald-Regular.ttf);
        }
    </style>
    <style type="text/css">
        .site-intro {
            position: relative;
            width: 100%;
            height: 50vh;
            overflow: hidden;
            box-shadow: -0.1rem 0 0.5rem 0 rgba(0, 0, 0, 0.5);
        }
        .site-intro-placeholder {
            position: absolute;
            z-index: -2;
            top: 0;
            left: 0px;
            width: calc(100% + 300px);
            height: 100%;
            background: repeating-linear-gradient(-45deg, #444 0, #444 80px, #333 80px, #333 160px);
            background-position: center center;
            transform: translate3d(-226px, 0, 0);
            animation: gradient-move 2.5s ease-out 0s 1;
        }
        @keyframes gradient-move {
        0% {
            transform: translate3d(-226px, 0, 0);
        }
        100% {
            transform: translate3d(0, 0, 0);
        }
    }
</style>
    <link rel="stylesheet" href = /css/style.css?v=20180120 />
    <script src="//cdn.staticfile.org/jquery/3.2.1/jquery.min.js" defer></script>
    
    <script src="/scripts/main.js" defer></script>
    <!-- 百度统计  -->
    
    <!-- 谷歌统计  -->
    
</head>

    
        <body class="post-body">
    
    
<header class="header">

    <div class="read-progress"></div>
    <div class="header-sidebar-menu">&#xe775;</div>
    <!-- post页的toggle banner  -->
    
    <div class="banner">
            <div class="blog-title">
                <a href="/" >SecretBase</a>
            </div>
            <div class="post-title">
                <a href="#" class="post-name">非洲人的战斗-DinosaurusLand</a>
            </div>
    </div>
    
    <a class="home-link" href=/>SecretBase</a>
</header>
    <div class="wrapper">
        <div class="site-intro">
    
    <!-- 主页  -->
    
    
    <!-- 404页  -->
            
    <div class="site-intro-img" style="background-image: url(http://oumn0o088.bkt.clouddn.com/post-bg.jpg)"></div>
    <div class="site-intro-placeholder"></div>
    <div class="site-intro-meta">
        <!-- 标题  -->
        <h1 class="intro-title">
            <!-- 主页  -->
            
            非洲人的战斗-DinosaurusLand
            <!-- 404 -->
            
        </h1>
        <!-- 副标题 -->
        <p class="intro-subtitle">
            <!-- 主页副标题  -->
            
            
            <!-- 404 -->
            
        </p>
        <!-- 文章页meta -->
        
            <!-- 文章页标签  -->
            
            <script>window._bd_share_config = { "common": { "bdSnsKey": {}, "bdText": "", "bdMini": "2", "bdMiniList": false, "bdPic": "", "bdStyle": "1", "bdSize": "16" }, "share": {} }; with (document) 0[(getElementsByTagName('head')[0] || body).appendChild(createElement('script')).src = "/static/api/js/share.js"];</script>
            <div class="post-intro-meta">
                <span class="post-intro-calander iconfont-archer">&#xe676;</span>
                <span class="post-intro-time">2018/03/20</span>
                <span id="busuanzi_container_page_pv" class="busuanzi-pv">
                    <span class="iconfont-archer">&#xe604;</span>
                    <span id="busuanzi_value_page_pv"></span>
                </span>
                <span class="shareWrapper">
                    <span class="iconfont-archer shareIcon">
                        &#xe601;
                    </span>
                    <span class="bdsharebuttonbox">
                        <a href="#" class="bds_more shareText" data-cmd="more">Share</a>
                    </span>
                </span>
            </div>
        
    </div>
</div>
        <img class="loading" src="/assets/loading.svg" style="display: block; margin: 6rem auto 0 auto; width: 6rem; height: 6rem;" />
        <script>
            var browser = {
                    versions: function () {
                        var u = window.navigator.userAgent;
                        return {
                            userAgent: u,
                            trident: u.indexOf('Trident') > -1, //IE内核
                            presto: u.indexOf('Presto') > -1, //opera内核
                            webKit: u.indexOf('AppleWebKit') > -1, //苹果、谷歌内核
                            gecko: u.indexOf('Gecko') > -1 && u.indexOf('KHTML') == -1, //火狐内核
                            mobile: !!u.match(/AppleWebKit.*Mobile.*/), //是否为移动终端
                            ios: !!u.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/), //ios终端
                            android: u.indexOf('Android') > -1 || u.indexOf('Linux') > -1, //android终端或者uc浏览器
                            iPhone: u.indexOf('iPhone') > -1 || u.indexOf('Mac') > -1, //是否为iPhone或者安卓QQ浏览器
                            iPad: u.indexOf('iPad') > -1, //是否为iPad
                            webApp: u.indexOf('Safari') == -1, //是否为web应用程序，没有头部与底部
                            weixin: u.indexOf('MicroMessenger') == -1, //是否为微信浏览器
                            uc: u.indexOf('UCBrowser') > -1 //是否为android下的UC浏览器
                        };
                    }()
                }

            function fontLoaded(){
                console.log('font loaded');
                if (document.getElementsByClassName('site-intro-meta')) {
                    document.getElementsByClassName('intro-title')[0].classList.add('intro-fade-in');
                    document.getElementsByClassName('intro-subtitle')[0].classList.add('intro-fade-in');
                    var postIntroTags = document.getElementsByClassName('post-intro-tags')[0],
                        postIntroMeat = document.getElementsByClassName('post-intro-meta')[0];
                        if (postIntroTags) {
                            postIntroTags.classList.add('post-fade-in');
                        }
                        if (postIntroMeat) {
                            postIntroMeat.classList.add('post-fade-in');
                        }
                    }
                }
                
            console.log("userAgent:" + browser.versions.userAgent);
            // UC不支持跨域，所以直接显示
            if (browser.versions.uc) {
                console.log("UCBrowser");
                fontLoaded();
            } else {
                WebFont.load({
                    custom: {
                        families: ['Oswald-Regular']
                    },
                    loading: function () {  //所有字体开始加载
                        // console.log('loading');
                    },
                    active: function () {  //所有字体已渲染
                        fontLoaded();
                    },
                    inactive: function () { //字体预加载失败，无效字体或浏览器不支持加载
                        console.log('inactive: timeout');
                        fontLoaded();
                    },
                    timeout: 7000 // Set the timeout to two seconds
                });
            }
        </script>
        <div class="container container-unloaded">
            <main class="main post-page">
    <article class="article-entry">
        <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> * </span><br><span class="line"><span class="keyword">import</span> random</span><br></pre></td></tr></table></figure>
<h1 id="Dataset-and-Preprocessing"><a href="#Dataset-and-Preprocessing" class="headerlink" title="Dataset and Preprocessing"></a>Dataset and Preprocessing</h1><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">data = open(<span class="string">'dinos.txt'</span>, <span class="string">'r'</span>).read()</span><br><span class="line">data= data.lower()</span><br><span class="line">chars = list(set(data))</span><br><span class="line">data_size, vocab_size = len(data), len(chars)</span><br><span class="line">print(<span class="string">'There are %d total characters and %d unique characters in your data.'</span> % ( data_size, vocab_size))</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">char_to_ix = &#123; ch:i <span class="keyword">for</span> i,ch <span class="keyword">in</span> enumerate(sorted(chars)) &#125; ix_to_char = &#123; i:ch <span class="keyword">for</span> i,ch <span class="keyword">in</span> enumerate(sorted(chars)) &#125; print(ix_to_char)</span><br></pre></td></tr></table></figure>
<h1 id="Building-blocks-of-the-model"><a href="#Building-blocks-of-the-model" class="headerlink" title="Building blocks of the model"></a>Building blocks of the model</h1><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### GRADED FUNCTION: clip</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clip</span><span class="params">(gradients, maxValue)</span>:</span> <span class="string">'''</span></span><br><span class="line"><span class="string">    Clips the gradients' values between minimum and maximum.</span></span><br><span class="line"><span class="string">Arguments:</span></span><br><span class="line"><span class="string">    gradients -- a dictionary containing the gradients "dWaa", "dWax", "dWya",</span></span><br><span class="line"><span class="string"> "db", "dby"</span></span><br><span class="line"><span class="string">    maxValue -- everything above this number is set to this number, and everythi</span></span><br><span class="line"><span class="string">ng less than -maxValue is set to -maxValue</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    gradients -- a dictionary with the clipped gradients.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    dWaa, dWax, dWya, db, dby = gradients[<span class="string">'dWaa'</span>], gradients[<span class="string">'dWax'</span>], gradients[</span><br><span class="line"><span class="string">'dWya'</span>], gradients[<span class="string">'db'</span>], gradients[<span class="string">'dby'</span>]</span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line"><span class="comment"># clip to mitigate exploding gradients, loop over [dWax, dWaa, dWya, db, db y]. (≈2 lines)</span></span><br><span class="line"><span class="keyword">for</span> gradient <span class="keyword">in</span> [dWax, dWaa, dWya, db, dby]: np.clip(gradient, -maxValue, maxValue, gradient)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">gradients = &#123;<span class="string">"dWaa"</span>: dWaa, <span class="string">"dWax"</span>: dWax, <span class="string">"dWya"</span>: dWya, <span class="string">"db"</span>: db, <span class="string">"dby"</span>: dby&#125; <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: sample</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span><span class="params">(parameters, char_to_ix, seed)</span>:</span> <span class="string">"""</span></span><br><span class="line"><span class="string">    Sample a sequence of characters according to a sequence of probability distr</span></span><br><span class="line"><span class="string">ibutions output of the RNN</span></span><br><span class="line"><span class="string">Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing the parameters Waa, Wax, Wya, by,</span></span><br><span class="line"><span class="string"> and b.</span></span><br><span class="line"><span class="string">    char_to_ix -- python dictionary mapping each character to an index.</span></span><br><span class="line"><span class="string">    seed -- used for grading purposes. Do not worry about it.</span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    indices -- a list of length n containing the indices of the sampled characte</span></span><br><span class="line"><span class="string">rs.</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">    <span class="comment"># Retrieve parameters and relevant shapes from "parameters" dictionary</span></span><br><span class="line">    Waa, Wax, Wya, by, b = parameters[<span class="string">'Waa'</span>], parameters[<span class="string">'Wax'</span>], parameters[<span class="string">'Wy</span></span><br><span class="line"><span class="string">a'</span>], parameters[<span class="string">'by'</span>], parameters[<span class="string">'b'</span>]</span><br><span class="line">    vocab_size = by.shape[<span class="number">0</span>]</span><br><span class="line">    n_a = Waa.shape[<span class="number">1</span>]</span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line"><span class="comment"># Step 1: Create the one-hot vector x for the first character (initializing the sequence generation). (≈1 line)</span></span><br><span class="line">    x = np.zeros((vocab_size, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># Step 1': Initialize a_prev as zeros (≈1 line)</span></span><br><span class="line">    a_prev = np.zeros((n_a, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># Create an empty list of indices, this is the list which will contain the l ist of indices of the characters to generate (≈1 line)</span></span><br><span class="line">indices = []</span><br><span class="line">    <span class="comment"># Idx is a flag to detect a newline character, we initialize it to -1</span></span><br><span class="line">idx = <span class="number">-1</span></span><br><span class="line">    <span class="comment"># Loop over time-steps t. At each time-step, sample a character from a proba</span></span><br><span class="line">bility distribution <span class="keyword">and</span> append</span><br><span class="line">    <span class="comment"># its index to "indices". We'll stop if we reach 50 characters (which should</span></span><br><span class="line"> be very unlikely <span class="keyword">with</span> a well</span><br><span class="line">    <span class="comment"># trained model), which helps debugging and prevents entering an infinite lo</span></span><br><span class="line">op.</span><br><span class="line">counter = <span class="number">0</span></span><br><span class="line">newline_character = char_to_ix[<span class="string">'\n'</span>]</span><br><span class="line"><span class="keyword">while</span> (idx != newline_character <span class="keyword">and</span> counter != <span class="number">50</span>):</span><br><span class="line">        <span class="comment"># Step 2: Forward propagate x using the equations (1), (2) and (3)</span></span><br><span class="line">        a = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b)</span><br><span class="line">        z = np.dot(Wya, a) + by</span><br><span class="line">        y = softmax(z)</span><br><span class="line">        <span class="comment"># for grading purposes</span></span><br><span class="line">        np.random.seed(counter+seed)</span><br><span class="line">        <span class="comment"># Step 3: Sample the index of a character within the vocabulary from the</span></span><br><span class="line"> probability distribution y</span><br><span class="line"> idx = np.random.choice(vocab_size, p=y.ravel())</span><br><span class="line">       <span class="comment"># Append the index to "indices"</span></span><br><span class="line">       indices.append(idx)</span><br><span class="line">       <span class="comment"># Step 4: Overwrite the input character as the one corresponding to the</span></span><br><span class="line">sampled index.</span><br><span class="line">       x = np.zeros((vocab_size, <span class="number">1</span>))</span><br><span class="line">       x[idx] = <span class="number">1</span></span><br><span class="line">       <span class="comment"># Update "a_prev" to be "a"</span></span><br><span class="line">a_prev = a</span><br><span class="line">       <span class="comment"># for grading purposes</span></span><br><span class="line">       seed += <span class="number">1</span></span><br><span class="line">       counter +=<span class="number">1</span></span><br><span class="line">   <span class="comment">### END CODE HERE ###</span></span><br><span class="line"><span class="keyword">if</span> (counter == <span class="number">50</span>): indices.append(char_to_ix[<span class="string">'\n'</span>])</span><br><span class="line"><span class="keyword">return</span> indices</span><br></pre></td></tr></table></figure>
<h1 id="Building-the-language-model"><a href="#Building-the-language-model" class="headerlink" title="Building the language model"></a>Building the language model</h1><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: optimize</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimize</span><span class="params">(X, Y, a_prev, parameters, learning_rate = <span class="number">0.01</span>)</span>:</span> <span class="string">"""</span></span><br><span class="line"><span class="string">    Execute one step of the optimization to train the model.</span></span><br><span class="line"><span class="string">Arguments:</span></span><br><span class="line"><span class="string">    X -- list of integers, where each integer is a number that maps to a charact</span></span><br><span class="line"><span class="string">er in the vocabulary.</span></span><br><span class="line"><span class="string">    Y -- list of integers, exactly the same as X but shifted one index to the le</span></span><br><span class="line"><span class="string">ft.</span></span><br><span class="line"><span class="string">    a_prev -- previous hidden state.</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        Wax -- Weight matrix multiplying the input, numpy array</span></span><br><span class="line"><span class="string"> of shape (n_a, n_x)</span></span><br><span class="line"><span class="string">                        Waa -- Weight matrix multiplying the hidden state, numpy</span></span><br><span class="line"><span class="string"> array of shape (n_a, n_a)</span></span><br><span class="line"><span class="string">                        Wya -- Weight matrix relating the hidden-state to the ou</span></span><br><span class="line"><span class="string">tput, numpy array of shape (n_y, n_a)</span></span><br><span class="line"><span class="string">                        b --  Bias, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        by -- Bias relating the hidden-state to the output, nump</span></span><br><span class="line"><span class="string">y array of shape (n_y, 1)</span></span><br><span class="line"><span class="string">    learning_rate -- learning rate for the model.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    loss -- value of the loss function (cross-entropy)</span></span><br><span class="line"><span class="string">    gradients -- python dictionary containing:</span></span><br><span class="line"><span class="string">(n_a, n_x)</span></span><br><span class="line"><span class="string">(n_a, n_a)</span></span><br><span class="line"><span class="string">(n_y, n_a)</span></span><br><span class="line"><span class="string">dWax -- Gradients of input-to-hidden weights, of shape</span></span><br><span class="line"><span class="string">dWaa -- Gradients of hidden-to-hidden weights, of shape</span></span><br><span class="line"><span class="string">dWya -- Gradients of hidden-to-output weights, of shape</span></span><br><span class="line"><span class="string">db -- Gradients of bias vector, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">dby -- Gradients of output bias vector, of shape (n_y,</span></span><br><span class="line"><span class="string">1)</span></span><br><span class="line"><span class="string">   a[len(X)-1] -- the last hidden state, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">   """</span></span><br><span class="line"><span class="comment">### START CODE HERE ###</span></span><br><span class="line"><span class="comment"># Forward propagate through time (≈1 line)</span></span><br><span class="line">   loss, cache = rnn_forward(X, Y, a_prev, parameters)</span><br><span class="line"><span class="comment"># Backpropagate through time (≈1 line)</span></span><br><span class="line">   gradients, a = rnn_backward(X, Y, parameters, cache)</span><br><span class="line"><span class="comment"># Clip your gradients between -5 (min) and 5 (max) (≈1 line)</span></span><br><span class="line">   gradients = clip(gradients, <span class="number">5</span>)</span><br><span class="line"><span class="comment"># Update parameters (≈1 line)</span></span><br><span class="line">   parameters = update_parameters(parameters, gradients, learning_rate)</span><br><span class="line">   <span class="comment">### END CODE HERE ###</span></span><br><span class="line"><span class="keyword">return</span> loss, gradients, a[len(X)<span class="number">-1</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: model</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(data, ix_to_char, char_to_ix, num_iterations = <span class="number">35000</span>, n_a = <span class="number">50</span>, dino_n ames = <span class="number">7</span>, vocab_size = <span class="number">27</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Trains the model and generates dinosaur names.</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    data -- text corpus</span></span><br><span class="line"><span class="string">    ix_to_char -- dictionary that maps the index to a character</span></span><br><span class="line"><span class="string">    char_to_ix -- dictionary that maps a character to an index</span></span><br><span class="line"><span class="string">    num_iterations -- number of iterations to train the model for</span></span><br><span class="line"><span class="string">    n_a -- number of units of the RNN cell</span></span><br><span class="line"><span class="string">    dino_names -- number of dinosaur names you want to sample at each iteration.</span></span><br><span class="line"><span class="string">    vocab_size -- number of unique characters found in the text, size of the voc</span></span><br><span class="line"><span class="string">abulary</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- learned parameters</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Retrieve n_x and n_y from vocab_size</span></span><br><span class="line">    n_x, n_y = vocab_size, vocab_size</span><br><span class="line">    <span class="comment"># Initialize parameters</span></span><br><span class="line">    parameters = initialize_parameters(n_a, n_x, n_y)</span><br><span class="line">    <span class="comment"># Initialize loss (this is required because we want to smooth our loss, do</span></span><br><span class="line">n<span class="string">'t worry about it)</span></span><br><span class="line"><span class="string">    loss = get_initial_loss(vocab_size, dino_names)</span></span><br><span class="line"><span class="string">    # Build list of all dinosaur names (training examples).</span></span><br><span class="line"><span class="string">with open("dinos.txt") as f: examples = f.readlines()</span></span><br><span class="line"><span class="string">examples = [x.lower().strip() for x in examples]</span></span><br><span class="line"><span class="string">    # Shuffle list of all dinosaur names</span></span><br><span class="line"><span class="string">    np.random.seed(0)</span></span><br><span class="line"><span class="string">    np.random.shuffle(examples)</span></span><br><span class="line"><span class="string">    # Initialize the hidden state of your LSTM</span></span><br><span class="line"><span class="string">    a_prev = np.zeros((n_a, 1))</span></span><br><span class="line"><span class="string">    # Optimization loop</span></span><br><span class="line"><span class="string">for j in range(num_iterations):</span></span><br><span class="line"><span class="string">        ### START CODE HERE ###</span></span><br><span class="line"><span class="string"># Use the hint above to define one training example (X,Y) (≈ 2 lines)</span></span><br><span class="line"><span class="string">index = j % len(examples)</span></span><br><span class="line"><span class="string">X = [None] + [char_to_ix[ch] for ch in examples[index]] Y = X[1:] + [char_to_ix["\n"]]</span></span><br><span class="line"><span class="string">        # Perform one optimization step: Forward-prop -&gt; Backward-prop -&gt; Clip -</span></span><br><span class="line"><span class="string">&gt; Update parameters</span></span><br><span class="line"><span class="string">        # Choose a learning rate of 0.01</span></span><br><span class="line"><span class="string">        curr_loss, gradients, a_prev = optimize(X, Y, a_prev, parameters, 0.01)</span></span><br><span class="line"><span class="string">  		### END CODE HERE ###</span></span><br><span class="line"><span class="string">        # Use a latency trick to keep the loss smooth. It happens here to accele</span></span><br><span class="line"><span class="string">rate the training.</span></span><br><span class="line"><span class="string">        loss = smooth(loss, curr_loss)</span></span><br><span class="line"><span class="string">        # Every 2000 Iteration, generate "n" characters thanks to sample() to ch</span></span><br><span class="line"><span class="string">eck if the model is learning properly</span></span><br><span class="line"><span class="string">if j % 2000 == 0:</span></span><br><span class="line"><span class="string">print('</span>Iteration: %d, Loss: %<span class="string">f' % (j, loss) + '</span>\n<span class="string">')</span></span><br><span class="line"><span class="string">            # The number of dinosaur names to print</span></span><br><span class="line"><span class="string">seed = 0</span></span><br><span class="line"><span class="string">for name in range(dino_names):</span></span><br><span class="line"><span class="string">                # Sample indices and print them</span></span><br><span class="line"><span class="string">                sampled_indices = sample(parameters, char_to_ix, seed)</span></span><br><span class="line"><span class="string">                print_sample(sampled_indices, ix_to_char)</span></span><br><span class="line"><span class="string">seed += 1 # To get the same result for grading purposed, increm ent the seed by one.</span></span><br><span class="line"><span class="string">print('</span>\n<span class="string">') return parameters</span></span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parameters = model(data, ix_to_char, char_to_ix)</span><br></pre></td></tr></table></figure>
<h1 id="Writing-like-Shakespeare"><a href="#Writing-like-Shakespeare" class="headerlink" title="Writing like Shakespeare"></a>Writing like Shakespeare</h1><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">from</span> keras.callbacks <span class="keyword">import</span> LambdaCallback</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model, load_model, Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, Activation, Dropout, Input, Masking <span class="keyword">from</span> keras.layers <span class="keyword">import</span> LSTM</span><br><span class="line"><span class="keyword">from</span> keras.utils.data_utils <span class="keyword">import</span> get_file</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing.sequence <span class="keyword">import</span> pad_sequences</span><br><span class="line"><span class="keyword">from</span> shakespeare_utils <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> io</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print_callback = LambdaCallback(on_epoch_end=on_epoch_end)</span><br><span class="line">model.fit(x, y, batch_size=<span class="number">128</span>, epochs=<span class="number">1</span>, callbacks=[print_callback])</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Run this cell to try with different inputs without having to re-train the mode l</span></span><br><span class="line">generate_output()</span><br></pre></td></tr></table></figure>
    </article>
    <!-- 前后页  -->
    <ul class="post-pager">
        
            <li class="next">
                <a href= "/2018/03/21/Jazz-with-LSTM/" title= Jazz_with_LSTM >
                    <span>Next Post</span>
                    <span>Jazz_with_LSTM</span>
                </a>
            </li>
        
        
            <li class="previous">
                <a href= "/2018/03/18/Build-RNN-firstly/" title= 非洲人的战斗-Build-RNN-firstly >
                    <span>Previous Post</span>
                    <span>非洲人的战斗-Build-RNN-firstly</span>
                </a>
            </li>
        
    </ul>
    <!-- 评论插件 -->
    <!-- 来必力City版安装代码 -->

<!-- City版安装代码已完成 -->
    
    
    <!--PC版-->

    <!--PC版-->


    
    <!-- 评论 -->
</main>
            <!-- profile -->
            
        </div>
        <footer class="footer footer-unloaded">
    <!-- social  -->
    
    <div class="social">
        
    
        
            
                <a href="mailto:758018147@qq.com" class="iconfont-archer email" title=email ></a>
            
        
    
        
    
        
    
        
            
                <a href="/atom.xml" class="iconfont-archer rss" target="_blank" title="rss"></a>
            
        
    

    </div>
    
    <!-- powered by Hexo  -->
    <div class="copyright">
        <span id="hexo-power">Powered by <a href="https://hexo.io/" target="_blank">Hexo</a></span><span class="iconfont-archer power">&#xe635;</span><span id="theme-info">Theme <a href="https://github.com/fi3ework/hexo-theme-archer" target="_blank">archer</a></span>
    </div>
    <!-- 不蒜子  -->
    
    <div class="busuanzi-container">
        <span id="busuanzi_container_site_pv">VISITOR VOLUME: <span id="busuanzi_value_site_pv"></span>
        </span>
    </div>
    
</footer>
    </div>
    <!-- toc -->
    
    <div class="toc-wrapper">
        <div class="toc-catalog">
            <span class="iconfont-archer catalog-icon">&#xe613;</span><span>CATALOG</span>
        </div>
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Dataset-and-Preprocessing"><span class="toc-number">1.</span> <span class="toc-text">Dataset and Preprocessing</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Building-blocks-of-the-model"><span class="toc-number">2.</span> <span class="toc-text">Building blocks of the model</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Building-the-language-model"><span class="toc-number">3.</span> <span class="toc-text">Building the language model</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Writing-like-Shakespeare"><span class="toc-number">4.</span> <span class="toc-text">Writing like Shakespeare</span></a></li></ol>
    </div>
    
    <div class="back-top">&#xe639;</div>
    <div class="sidebar">
    <div class="sidebar-header sidebar-header-show-archive">
        <div class="sidebar-category">
            <span class="sidebar-archive-link"><span class="iconfont-archer">&#xe67d;</span>Archive</span>
            <span class="sidebar-tags-link"><span class="iconfont-archer">&#xe610;</span>Tag</span>
        </div>
    </div>
    <div class="sidebar-content sidebar-content-show-archive">
          <div class="sidebar-archive">
    <!-- 在ejs中将archive按照时间排序 -->
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <div class="total-archive"> Total : 18 </div>
    
    <div class="post-archive">
    
    
    
    
    <div class="archive-year"> 2018 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/24</span><a class="archive-post-title" href= "/2018/03/24/emojify/" >非洲人的战斗-emojify</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/22</span><a class="archive-post-title" href= "/2018/03/22/word-vectors/" >非洲人的战斗-word_vectors</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/21</span><a class="archive-post-title" href= "/2018/03/21/Jazz-with-LSTM/" >Jazz_with_LSTM</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/20</span><a class="archive-post-title" href= "/2018/03/20/DinosaurusLand/" >非洲人的战斗-DinosaurusLand</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/18</span><a class="archive-post-title" href= "/2018/03/18/Build-RNN-firstly/" >非洲人的战斗-Build-RNN-firstly</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/28</span><a class="archive-post-title" href= "/2018/02/28/neural-style-transfer/" >非洲人的战斗-neural_style_transfer</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/26</span><a class="archive-post-title" href= "/2018/02/26/car-detection/" >非洲人的战斗-car_detection</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/14</span><a class="archive-post-title" href= "/2018/02/14/ResidualNetworks/" >非洲人的战斗-ResidualNetworks</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/13</span><a class="archive-post-title" href= "/2018/02/13/CNN-Application/" >非洲人的战斗-CNN_Application</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/12</span><a class="archive-post-title" href= "/2018/02/12/Build-CNN-firstly/" >非洲人的战斗Build_CNN_firstly</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/08</span><a class="archive-post-title" href= "/2018/02/08/optimization/" >非洲人的战斗-Optimization Method</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/08</span><a class="archive-post-title" href= "/2018/02/08/GradientChecking/" >非洲人的战斗-GradientChecking</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/08</span><a class="archive-post-title" href= "/2018/02/08/Regularization/" >非洲人的战斗-Regularization</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/08</span><a class="archive-post-title" href= "/2018/02/08/Initialization/" >非洲人的战斗-Initialization</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/07</span><a class="archive-post-title" href= "/2018/02/07/DNN-image-classification/" >非洲人的战斗-DNN的图片分类</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/06</span><a class="archive-post-title" href= "/2018/02/06/Build_DNN_firstly/" >非洲人的战斗-building DNN firstly</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/05</span><a class="archive-post-title" href= "/2018/02/05/one-hidden-layer/" >非洲人的战斗-one_hidden_layer</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">01/30</span><a class="archive-post-title" href= "/2018/01/30/hello-world/" >Hello World</a>
        </li>
    
    </div>
  </div>
        <div class="sidebar-tags">
    <div class="sidebar-tags-name">
    
        <span class="sidebar-tag-name"><a href= "#">Tensorflow</a></span>
    
    </div>
    <div class="tag-load-fail" style="display: none; color: #ccc; font-size: 0.6rem;">
    缺失模块。<br/>
    1、请确保node版本大于6.2<br/>
    2、在博客根目录（注意不是archer根目录）执行以下命令：<br/>
    <span style="color: #f75357; font-size: 1rem; line-height: 2rem;">npm i hexo-generator-json-content --save</span><br/>
    3、在根目录_config.yml里添加配置：
    <pre style="color: #787878; font-size: 0.6rem;">
jsonContent:
  meta: false
  pages: false
  posts:
    title: true
    date: true
    path: true
    text: false
    raw: false
    content: false
    slug: false
    updated: false
    comments: false
    link: false
    permalink: false
    excerpt: false
    categories: false
    tags: true</pre>
    </div> 
    <div class="sidebar-tag-list"></div>
</div>
    </div>
</div> 
    <script>
    var jsInfo = {
        root: '/'
    }
</script>
    <!-- 不蒜子  -->
    
    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    
    <!-- CNZZ统计  -->
    
    </div>
    </body>
</html>


