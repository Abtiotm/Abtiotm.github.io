<!DOCTYPE html>
<html>
    <head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" >
    <title>
        
        非洲人的战斗-building DNN firstly · SecretBase
        
    </title>
    <link rel="icon" href= /assets/favicon.ico>
    <!-- TODO: 在font-face加载完毕后改变字体  -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/webfont/1.6.28/webfontloader.js"></script>
    <!-- 提前加载place holder  -->
    <style type="text/css">
        @font-face {
            font-family: 'Oswald-Regular';
            src: url(/font/Oswald-Regular.ttf);
        }
    </style>
    <style type="text/css">
        .site-intro {
            position: relative;
            width: 100%;
            height: 50vh;
            overflow: hidden;
            box-shadow: -0.1rem 0 0.5rem 0 rgba(0, 0, 0, 0.5);
        }
        .site-intro-placeholder {
            position: absolute;
            z-index: -2;
            top: 0;
            left: 0px;
            width: calc(100% + 300px);
            height: 100%;
            background: repeating-linear-gradient(-45deg, #444 0, #444 80px, #333 80px, #333 160px);
            background-position: center center;
            transform: translate3d(-226px, 0, 0);
            animation: gradient-move 2.5s ease-out 0s 1;
        }
        @keyframes gradient-move {
        0% {
            transform: translate3d(-226px, 0, 0);
        }
        100% {
            transform: translate3d(0, 0, 0);
        }
    }
</style>
    <link rel="stylesheet" href = /css/style.css?v=20180120 />
    <script src="//cdn.staticfile.org/jquery/3.2.1/jquery.min.js" defer></script>
    
    <script src="/scripts/main.js" defer></script>
    <!-- 百度统计  -->
    
    <!-- 谷歌统计  -->
    
</head>

    
        <body class="post-body">
    
    
<header class="header">

    <div class="read-progress"></div>
    <div class="header-sidebar-menu">&#xe775;</div>
    <!-- post页的toggle banner  -->
    
    <div class="banner">
            <div class="blog-title">
                <a href="/" >SecretBase</a>
            </div>
            <div class="post-title">
                <a href="#" class="post-name">非洲人的战斗-building DNN firstly</a>
            </div>
    </div>
    
    <a class="home-link" href=/>SecretBase</a>
</header>
    <div class="wrapper">
        <div class="site-intro">
    
    <!-- 主页  -->
    
    
    <!-- 404页  -->
            
    <div class="site-intro-img" style="background-image: url(http://oumn0o088.bkt.clouddn.com/post-bg.jpg)"></div>
    <div class="site-intro-placeholder"></div>
    <div class="site-intro-meta">
        <!-- 标题  -->
        <h1 class="intro-title">
            <!-- 主页  -->
            
            非洲人的战斗-building DNN firstly
            <!-- 404 -->
            
        </h1>
        <!-- 副标题 -->
        <p class="intro-subtitle">
            <!-- 主页副标题  -->
            
            
            <!-- 404 -->
            
        </p>
        <!-- 文章页meta -->
        
            <!-- 文章页标签  -->
            
            <script>window._bd_share_config = { "common": { "bdSnsKey": {}, "bdText": "", "bdMini": "2", "bdMiniList": false, "bdPic": "", "bdStyle": "1", "bdSize": "16" }, "share": {} }; with (document) 0[(getElementsByTagName('head')[0] || body).appendChild(createElement('script')).src = "/static/api/js/share.js"];</script>
            <div class="post-intro-meta">
                <span class="post-intro-calander iconfont-archer">&#xe676;</span>
                <span class="post-intro-time">2018/02/06</span>
                <span id="busuanzi_container_page_pv" class="busuanzi-pv">
                    <span class="iconfont-archer">&#xe604;</span>
                    <span id="busuanzi_value_page_pv"></span>
                </span>
                <span class="shareWrapper">
                    <span class="iconfont-archer shareIcon">
                        &#xe601;
                    </span>
                    <span class="bdsharebuttonbox">
                        <a href="#" class="bds_more shareText" data-cmd="more">Share</a>
                    </span>
                </span>
            </div>
        
    </div>
</div>
        <img class="loading" src="/assets/loading.svg" style="display: block; margin: 6rem auto 0 auto; width: 6rem; height: 6rem;" />
        <script>
            var browser = {
                    versions: function () {
                        var u = window.navigator.userAgent;
                        return {
                            userAgent: u,
                            trident: u.indexOf('Trident') > -1, //IE内核
                            presto: u.indexOf('Presto') > -1, //opera内核
                            webKit: u.indexOf('AppleWebKit') > -1, //苹果、谷歌内核
                            gecko: u.indexOf('Gecko') > -1 && u.indexOf('KHTML') == -1, //火狐内核
                            mobile: !!u.match(/AppleWebKit.*Mobile.*/), //是否为移动终端
                            ios: !!u.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/), //ios终端
                            android: u.indexOf('Android') > -1 || u.indexOf('Linux') > -1, //android终端或者uc浏览器
                            iPhone: u.indexOf('iPhone') > -1 || u.indexOf('Mac') > -1, //是否为iPhone或者安卓QQ浏览器
                            iPad: u.indexOf('iPad') > -1, //是否为iPad
                            webApp: u.indexOf('Safari') == -1, //是否为web应用程序，没有头部与底部
                            weixin: u.indexOf('MicroMessenger') == -1, //是否为微信浏览器
                            uc: u.indexOf('UCBrowser') > -1 //是否为android下的UC浏览器
                        };
                    }()
                }

            function fontLoaded(){
                console.log('font loaded');
                if (document.getElementsByClassName('site-intro-meta')) {
                    document.getElementsByClassName('intro-title')[0].classList.add('intro-fade-in');
                    document.getElementsByClassName('intro-subtitle')[0].classList.add('intro-fade-in');
                    var postIntroTags = document.getElementsByClassName('post-intro-tags')[0],
                        postIntroMeat = document.getElementsByClassName('post-intro-meta')[0];
                        if (postIntroTags) {
                            postIntroTags.classList.add('post-fade-in');
                        }
                        if (postIntroMeat) {
                            postIntroMeat.classList.add('post-fade-in');
                        }
                    }
                }
                
            console.log("userAgent:" + browser.versions.userAgent);
            // UC不支持跨域，所以直接显示
            if (browser.versions.uc) {
                console.log("UCBrowser");
                fontLoaded();
            } else {
                WebFont.load({
                    custom: {
                        families: ['Oswald-Regular']
                    },
                    loading: function () {  //所有字体开始加载
                        // console.log('loading');
                    },
                    active: function () {  //所有字体已渲染
                        fontLoaded();
                    },
                    inactive: function () { //字体预加载失败，无效字体或浏览器不支持加载
                        console.log('inactive: timeout');
                        fontLoaded();
                    },
                    timeout: 7000 // Set the timeout to two seconds
                });
            }
        </script>
        <div class="container container-unloaded">
            <main class="main post-page">
    <article class="article-entry">
        <h1 id="1-Package"><a href="#1-Package" class="headerlink" title="1.Package"></a>1.Package</h1><p>深度神经网络第一步，导入必要的包。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py             <span class="comment">#什么东西</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> textCases_v3 <span class="keyword">import</span> *            <span class="comment">#为了测试</span></span><br><span class="line"><span class="keyword">from</span> dnn_utils_v2 <span class="keyword">import</span> sigmoid,sigmoid_backward,relu,relu_backward   <span class="comment">#必要的function</span></span><br><span class="line"></span><br><span class="line">%matplotlib inline                    <span class="comment">#将图表嵌入notebook</span></span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">5.0</span>,<span class="number">4.0</span>)</span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span>    <span class="comment">#神奇的操作</span></span><br><span class="line"></span><br><span class="line">%load_ext autoreload         <span class="comment">#在执行用户代码前，重新装入软件的扩展和模块。</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">1</span>)    <span class="comment">#是随机函数调用恒定</span></span><br></pre></td></tr></table></figure>
<h1 id="2-必要的解释（略）"><a href="#2-必要的解释（略）" class="headerlink" title="2.必要的解释（略）"></a>2.必要的解释（略）</h1><h1 id="3-Initialization"><a href="#3-Initialization" class="headerlink" title="3.Initialization"></a>3.Initialization</h1><h3 id="3-1-两层神经网络"><a href="#3-1-两层神经网络" class="headerlink" title="3.1-两层神经网络"></a>3.1-两层神经网络</h3><p>模型结构：LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">(n_x,n_h,n_y)</span>:</span></span><br><span class="line"></span><br><span class="line">  np.random.seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">  W1 = np.random.randn(n_h,n_x) * <span class="number">.01</span>       <span class="comment">#使之在0～0.01</span></span><br><span class="line">  b1 = np.zeros((n_h,<span class="number">1</span>))</span><br><span class="line">  W2 = np.random.randn((n_y,n_h)) * <span class="number">.01</span></span><br><span class="line">  b2 = np.zeros((n_y,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="keyword">assert</span>(W1.shape == (n_h,n_x))</span><br><span class="line">  <span class="keyword">assert</span>(b1.shape == (n_h,<span class="number">1</span>))</span><br><span class="line">  <span class="keyword">assert</span>(W1.shape == (n_y,n_h))</span><br><span class="line">  <span class="keyword">assert</span>(W1.shape == (n_y,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">  parameters = &#123;<span class="string">"W1"</span> : W1, <span class="string">"b1"</span> : b1, <span class="string">"W2"</span> : W2, <span class="string">"b2"</span> : b2&#125;</span><br><span class="line">  <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure></p>
<h3 id="3-2-L层深度神经网络"><a href="#3-2-L层深度神经网络" class="headerlink" title="3.2-L层深度神经网络"></a>3.2-L层深度神经网络</h3><p>模型结构：[LINEAR -&gt; RELU]X(L-1) -&gt; LINEAR -&gt; SIGMOID</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_deep</span><span class="params">(layers_dims)</span>:</span></span><br><span class="line">  <span class="string">"""</span></span><br><span class="line"><span class="string">  layers_dims,一个存放着每层dimensions的列表</span></span><br><span class="line"><span class="string">  Wl.shape = (layers_dims[l],layers_dims[L-1])</span></span><br><span class="line"><span class="string">  bl.shape = (layers_dims[l],1)</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line"></span><br><span class="line">  np.random.seed(<span class="number">3</span>)</span><br><span class="line">  parameters = &#123;&#125;</span><br><span class="line">  L = len(layers_dims)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>,L):</span><br><span class="line">    parameters[<span class="string">'W'</span>+str(l)] = np.random.randn(layers_dims[l],layers_dims[l<span class="number">-1</span>]) * <span class="number">.01</span></span><br><span class="line">    parameters[<span class="string">'b'</span>+str(l)] = np.zeros((layers_dims[l],<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(parameters[<span class="string">'W'</span>+str(l)].shape == (layers_dims[l],layers_dims[l<span class="number">-1</span>]))</span><br><span class="line">    <span class="keyword">assert</span>(parameters[<span class="string">'b'</span>+str(l)].shape == (layers_dims[l],<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>
<h1 id="4-Forward-propagation-module"><a href="#4-Forward-propagation-module" class="headerlink" title="4.Forward propagation module"></a>4.Forward propagation module</h1><h3 id="4-1-Linear-Forward"><a href="#4-1-Linear-Forward" class="headerlink" title="4.1-Linear Forward"></a>4.1-Linear Forward</h3><ul>
<li>LINEAR</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_forward</span><span class="params">(A,W,b)</span>:</span></span><br><span class="line">  <span class="string">"""</span></span><br><span class="line"><span class="string">  A --(previous layer, number of examples)</span></span><br><span class="line"><span class="string">  W --(current layer,previous layer)</span></span><br><span class="line"><span class="string">  b --(current layer,1)</span></span><br><span class="line"><span class="string">  Z :pre-activation parameter</span></span><br><span class="line"><span class="string">  cache:为反向传播计算存储</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line"></span><br><span class="line">  Z = np.dot(W,A)+b</span><br><span class="line"></span><br><span class="line">  <span class="keyword">assert</span>(Z.shape == (W.shape[<span class="number">0</span>],A.shape[<span class="number">1</span>]))</span><br><span class="line">  cache = (A,W,b)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> Z,cache</span><br></pre></td></tr></table></figure>
<h3 id="4-2-Linear-linear-forward"><a href="#4-2-Linear-linear-forward" class="headerlink" title="4.2-Linear linear_forward"></a>4.2-Linear linear_forward</h3><ul>
<li>LINEAR-&gt;ACTIVATION</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_forward</span><span class="params">(A_prev,W,b,activation)</span>:</span></span><br><span class="line">  <span class="keyword">if</span> activation == <span class="string">"sigmoid"</span>:</span><br><span class="line">    Z,linear_cache = linear_forward(A_prev,W,b)</span><br><span class="line"></span><br><span class="line">    A,activation_cache = sigmoid(Z)    <span class="comment">#此处不明</span></span><br><span class="line">  <span class="keyword">elif</span> activation == <span class="string">"relu"</span>:</span><br><span class="line">    Z,linear_cache = linear_forward(A_prev,W,b)</span><br><span class="line"></span><br><span class="line">    A,activation_cache = relu(Z)    <span class="comment">#此处不明</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">assert</span>(A.shape == (W.shape[<span class="number">0</span>],A_prev.shape[<span class="number">1</span>]))</span><br><span class="line">  cache = (linear_cache,activation_cache)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> A,cache</span><br></pre></td></tr></table></figure>
<h3 id="4-3-L-Layer-Model"><a href="#4-3-L-Layer-Model" class="headerlink" title="4.3-L-Layer Model"></a>4.3-L-Layer Model</h3><ul>
<li>[LINEAR-&gt;RELU]x(L-1) -&gt; LINEAR -&gt; SIGMOID</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_forward</span><span class="params">(X,parameters)</span>:</span></span><br><span class="line"></span><br><span class="line">  caches = []</span><br><span class="line">  A = X</span><br><span class="line">  L = len(parameters)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>,L):</span><br><span class="line">    A_prev = A</span><br><span class="line"></span><br><span class="line">    AL,cache = linear_activation_forward(A_prev,parameters[<span class="string">"w"</span>+str(l)],parameters[<span class="string">"b"</span>+str(l)],activation=<span class="string">"relu"</span>)</span><br><span class="line">    caches.append(cache)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(AL.shape == (<span class="number">1</span>,X.shape[<span class="number">1</span>]))    <span class="comment">#zmzhuone?</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> AL,caches</span><br></pre></td></tr></table></figure>
<h1 id="5-Cost-function"><a href="#5-Cost-function" class="headerlink" title="5.Cost function"></a>5.Cost function</h1><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(AL,Y)</span>:</span></span><br><span class="line">  m = Y.shape[<span class="number">1</span>]</span><br><span class="line">  cost = (<span class="number">1.</span>/m) * (-np.dot(Y,np.log(AL).T) - np.dot(<span class="number">1</span>-Y,np.log(<span class="number">1</span>-AL).T))</span><br><span class="line"></span><br><span class="line">  cost = np.squeeze(cost)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">assert</span>(cost.shape == ())</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure>
<h1 id="6-Backward-propagation-module"><a href="#6-Backward-propagation-module" class="headerlink" title="6.Backward propagation module"></a>6.Backward propagation module</h1><h3 id="6-1-Linear-backward"><a href="#6-1-Linear-backward" class="headerlink" title="6.1-Linear backward"></a>6.1-Linear backward</h3><ul>
<li>LINEAR backward</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_backward</span><span class="params">(dZ,cache)</span>:</span></span><br><span class="line">  A_prev,W,b = cache</span><br><span class="line">  m = A_prev.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">  dW = np.dot(dZ,A_prev.T) / m</span><br><span class="line">  db = np.sum(dZ,axis=<span class="number">1</span>,keepdims = <span class="keyword">True</span>) / m</span><br><span class="line">  dA_prev = np.dot(W.T,dZ)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">assert</span>(dA_prev.shape == A_prev.shape)</span><br><span class="line">  <span class="keyword">assert</span>(dW.shape == W.shape)</span><br><span class="line">  <span class="keyword">assert</span>(db.shape == b.shape)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> dA_prev,dW,db</span><br></pre></td></tr></table></figure>
<h3 id="6-2-Linear-Action-backward"><a href="#6-2-Linear-Action-backward" class="headerlink" title="6.2-Linear-Action backward"></a>6.2-Linear-Action backward</h3><ul>
<li>LINEAR -&gt; ACTIVATION backward<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_backward</span><span class="params">(dA,cache,activation)</span>:</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> activation == <span class="string">"relu"</span>:</span><br><span class="line">    dZ = relu_backward(dA,activation_cache)</span><br><span class="line">    dA_prev,dW,db = linear_backward(dZ,linear_cache)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">elif</span> activation == <span class="string">"sigmoid"</span>:</span><br><span class="line">    dZ = sigmoid_backward(dA,activation_cache)</span><br><span class="line">    dA_prev,dW,db = linear_backward(dZ,linear_cache)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> dA_prev,dW,db</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="6-3-L-Model-Backward"><a href="#6-3-L-Model-Backward" class="headerlink" title="6.3-L-Model Backward"></a>6.3-L-Model Backward</h3><ul>
<li>[LINEAR-&gt;RELU]x(L-1)-&gt;LINEAR-&gt;SIGMOID backward</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_backward</span><span class="params">(AL,Y,caches)</span>:</span></span><br><span class="line">  grads = &#123;&#125;</span><br><span class="line">  L = len(caches)</span><br><span class="line">  m = AL.shape[l]</span><br><span class="line">  Y = Y.reshape(AL.shape)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  dAL = - (np.divide(Y,AL) - np.divide(<span class="number">1</span>-Y,<span class="number">1</span>-AL))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  current_cache = caches[L<span class="number">-1</span>]</span><br><span class="line">  grads[<span class="string">"dA"</span>+str(L)], grads[<span class="string">"dW"</span>+str(L)], grads[<span class="string">"db"</span>+str(L)] = linear_activation_backward(dAL,current_cache,activation=<span class="string">"sigmoid"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> l <span class="keyword">in</span> reversed(range(L<span class="number">-1</span>)):</span><br><span class="line">    current_cache = caches[l]</span><br><span class="line">    dA_prev_temp,dW_temp,db_temp = linear_activation_backward(grads[<span class="string">"dA"</span>+str(l+<span class="number">2</span>)],current_cache,activation=<span class="string">"relu"</span>)</span><br><span class="line">    grads[<span class="string">"dA"</span>+str(l+<span class="number">1</span>)] = dA_prev_temp</span><br><span class="line">    grads[<span class="string">"dW"</span>+str(l+<span class="number">1</span>)] = dW_temp</span><br><span class="line">    grads[<span class="string">"db"</span>+str(l+<span class="number">1</span>)] = db_temp</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure>
<h3 id="6-4-Update-Parameters"><a href="#6-4-Update-Parameters" class="headerlink" title="6.4-Update Parameters"></a>6.4-Update Parameters</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters,grads,learning_rate)</span>:</span></span><br><span class="line">  L = len(parameters)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line">    parameters[<span class="string">"W"</span>+str(l+<span class="number">1</span>)] = parameters[<span class="string">"W"</span>+str(l+<span class="number">1</span>)] - learning_rate * grads[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)]</span><br><span class="line">    parameters[<span class="string">"b"</span>+str(l+<span class="number">1</span>)] = parameters[<span class="string">"b"</span>+str(l+<span class="number">1</span>)] - learning_rate * grads[<span class="string">"db"</span> + str(l+<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>

    </article>
    <!-- 前后页  -->
    <ul class="post-pager">
        
            <li class="next">
                <a href= "/2018/02/07/DNN-image-classification/" title= 非洲人的战斗-DNN的图片分类 >
                    <span>Next Post</span>
                    <span>非洲人的战斗-DNN的图片分类</span>
                </a>
            </li>
        
        
            <li class="previous">
                <a href= "/2018/02/05/one-hidden-layer/" title= 非洲人的战斗-one_hidden_layer >
                    <span>Previous Post</span>
                    <span>非洲人的战斗-one_hidden_layer</span>
                </a>
            </li>
        
    </ul>
    <!-- 评论插件 -->
    <!-- 来必力City版安装代码 -->

<!-- City版安装代码已完成 -->
    
    
    <!--PC版-->

    <!--PC版-->


    
    <!-- 评论 -->
</main>
            <!-- profile -->
            
        </div>
        <footer class="footer footer-unloaded">
    <!-- social  -->
    
    <div class="social">
        
    
        
            
                <a href="mailto:758018147@qq.com" class="iconfont-archer email" title=email ></a>
            
        
    
        
    
        
    
        
            
                <a href="/atom.xml" class="iconfont-archer rss" target="_blank" title="rss"></a>
            
        
    

    </div>
    
    <!-- powered by Hexo  -->
    <div class="copyright">
        <span id="hexo-power">Powered by <a href="https://hexo.io/" target="_blank">Hexo</a></span><span class="iconfont-archer power">&#xe635;</span><span id="theme-info">Theme <a href="https://github.com/fi3ework/hexo-theme-archer" target="_blank">archer</a></span>
    </div>
    <!-- 不蒜子  -->
    
    <div class="busuanzi-container">
        <span id="busuanzi_container_site_pv">VISITOR VOLUME: <span id="busuanzi_value_site_pv"></span>
        </span>
    </div>
    
</footer>
    </div>
    <!-- toc -->
    
    <div class="toc-wrapper">
        <div class="toc-catalog">
            <span class="iconfont-archer catalog-icon">&#xe613;</span><span>CATALOG</span>
        </div>
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-Package"><span class="toc-number">1.</span> <span class="toc-text">1.Package</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-必要的解释（略）"><span class="toc-number">2.</span> <span class="toc-text">2.必要的解释（略）</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-Initialization"><span class="toc-number">3.</span> <span class="toc-text">3.Initialization</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-两层神经网络"><span class="toc-number">3.0.1.</span> <span class="toc-text">3.1-两层神经网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-L层深度神经网络"><span class="toc-number">3.0.2.</span> <span class="toc-text">3.2-L层深度神经网络</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-Forward-propagation-module"><span class="toc-number">4.</span> <span class="toc-text">4.Forward propagation module</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-Linear-Forward"><span class="toc-number">4.0.1.</span> <span class="toc-text">4.1-Linear Forward</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-Linear-linear-forward"><span class="toc-number">4.0.2.</span> <span class="toc-text">4.2-Linear linear_forward</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-L-Layer-Model"><span class="toc-number">4.0.3.</span> <span class="toc-text">4.3-L-Layer Model</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-Cost-function"><span class="toc-number">5.</span> <span class="toc-text">5.Cost function</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6-Backward-propagation-module"><span class="toc-number">6.</span> <span class="toc-text">6.Backward propagation module</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-Linear-backward"><span class="toc-number">6.0.1.</span> <span class="toc-text">6.1-Linear backward</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-Linear-Action-backward"><span class="toc-number">6.0.2.</span> <span class="toc-text">6.2-Linear-Action backward</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-3-L-Model-Backward"><span class="toc-number">6.0.3.</span> <span class="toc-text">6.3-L-Model Backward</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-4-Update-Parameters"><span class="toc-number">6.0.4.</span> <span class="toc-text">6.4-Update Parameters</span></a></li></ol></li></ol></li></ol>
    </div>
    
    <div class="back-top">&#xe639;</div>
    <div class="sidebar">
    <div class="sidebar-header sidebar-header-show-archive">
        <div class="sidebar-category">
            <span class="sidebar-archive-link"><span class="iconfont-archer">&#xe67d;</span>Archive</span>
            <span class="sidebar-tags-link"><span class="iconfont-archer">&#xe610;</span>Tag</span>
        </div>
    </div>
    <div class="sidebar-content sidebar-content-show-archive">
          <div class="sidebar-archive">
    <!-- 在ejs中将archive按照时间排序 -->
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <div class="total-archive"> Total : 20 </div>
    
    <div class="post-archive">
    
    
    
    
    <div class="archive-year"> 2018 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/27</span><a class="archive-post-title" href= "/2018/03/27/trigger-word-detection/" >非洲人的战斗-trigger_word_detection</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/26</span><a class="archive-post-title" href= "/2018/03/26/machine-translation/" >非洲人的战斗-machine_translation</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/24</span><a class="archive-post-title" href= "/2018/03/24/emojify/" >非洲人的战斗-emojify</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/22</span><a class="archive-post-title" href= "/2018/03/22/word-vectors/" >非洲人的战斗-word_vectors</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/21</span><a class="archive-post-title" href= "/2018/03/21/Jazz-with-LSTM/" >非洲人的战斗-Jazz_with_LSTM</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/20</span><a class="archive-post-title" href= "/2018/03/20/DinosaurusLand/" >非洲人的战斗-DinosaurusLand</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/18</span><a class="archive-post-title" href= "/2018/03/18/Build-RNN-firstly/" >非洲人的战斗-Build-RNN-firstly</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/28</span><a class="archive-post-title" href= "/2018/02/28/neural-style-transfer/" >非洲人的战斗-neural_style_transfer</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/26</span><a class="archive-post-title" href= "/2018/02/26/car-detection/" >非洲人的战斗-car_detection</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/14</span><a class="archive-post-title" href= "/2018/02/14/ResidualNetworks/" >非洲人的战斗-ResidualNetworks</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/13</span><a class="archive-post-title" href= "/2018/02/13/CNN-Application/" >非洲人的战斗-CNN_Application</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/12</span><a class="archive-post-title" href= "/2018/02/12/Build-CNN-firstly/" >非洲人的战斗Build_CNN_firstly</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/08</span><a class="archive-post-title" href= "/2018/02/08/optimization/" >非洲人的战斗-Optimization Method</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/08</span><a class="archive-post-title" href= "/2018/02/08/GradientChecking/" >非洲人的战斗-GradientChecking</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/08</span><a class="archive-post-title" href= "/2018/02/08/Regularization/" >非洲人的战斗-Regularization</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/08</span><a class="archive-post-title" href= "/2018/02/08/Initialization/" >非洲人的战斗-Initialization</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/07</span><a class="archive-post-title" href= "/2018/02/07/DNN-image-classification/" >非洲人的战斗-DNN的图片分类</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/06</span><a class="archive-post-title" href= "/2018/02/06/Build_DNN_firstly/" >非洲人的战斗-building DNN firstly</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/05</span><a class="archive-post-title" href= "/2018/02/05/one-hidden-layer/" >非洲人的战斗-one_hidden_layer</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">01/30</span><a class="archive-post-title" href= "/2018/01/30/hello-world/" >Hello World</a>
        </li>
    
    </div>
  </div>
        <div class="sidebar-tags">
    <div class="sidebar-tags-name">
    
        <span class="sidebar-tag-name"><a href= "#">Tensorflow</a></span>
    
    </div>
    <div class="tag-load-fail" style="display: none; color: #ccc; font-size: 0.6rem;">
    缺失模块。<br/>
    1、请确保node版本大于6.2<br/>
    2、在博客根目录（注意不是archer根目录）执行以下命令：<br/>
    <span style="color: #f75357; font-size: 1rem; line-height: 2rem;">npm i hexo-generator-json-content --save</span><br/>
    3、在根目录_config.yml里添加配置：
    <pre style="color: #787878; font-size: 0.6rem;">
jsonContent:
  meta: false
  pages: false
  posts:
    title: true
    date: true
    path: true
    text: false
    raw: false
    content: false
    slug: false
    updated: false
    comments: false
    link: false
    permalink: false
    excerpt: false
    categories: false
    tags: true</pre>
    </div> 
    <div class="sidebar-tag-list"></div>
</div>
    </div>
</div> 
    <script>
    var jsInfo = {
        root: '/'
    }
</script>
    <!-- 不蒜子  -->
    
    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    
    <!-- CNZZ统计  -->
    
    </div>
    </body>
</html>


